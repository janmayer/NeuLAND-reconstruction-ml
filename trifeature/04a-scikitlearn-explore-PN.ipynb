{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiplicity determination with Scikit-learn classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we test out scikit-learn classification models for multiplicity reconstruction with the three main features number of hits, number of clusters, and total deposited energy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"nPN\"\n",
    "nmax = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn import *\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from helpers import tridata, with_timeout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load ALL the classification models from scikit-learn.\n",
    "Note that some models are very slow to train with large datasets or crash outright, so we give them a reduced number of (shuffled) rows to learn.\n",
    "Note that `n_jobs=1` is used, as parallelism is introduced later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    (\"AdaBoostClassifier\", sklearn.ensemble.AdaBoostClassifier(), \"fast\"),\n",
    "    (\"BaggingClassifier\", sklearn.ensemble.BaggingClassifier(n_jobs=1), \"fast\"),\n",
    "    (\"BernoulliNB\", sklearn.naive_bayes.BernoulliNB(), \"fast\"),\n",
    "    (\"CalibratedClassifierCV\", sklearn.calibration.CalibratedClassifierCV(cv=5), \"slow\"),\n",
    "    (\"ComplementNB\", sklearn.naive_bayes.ComplementNB(), \"fast\"),\n",
    "    (\"DecisionTreeClassifier\", sklearn.tree.DecisionTreeClassifier(), \"fast\"),\n",
    "    (\"ExtraTreeClassifier\", sklearn.tree.ExtraTreeClassifier(), \"fast\"),\n",
    "    (\"ExtraTreesClassifier\", sklearn.ensemble.ExtraTreesClassifier(n_estimators=100, n_jobs=1), \"medi\"),\n",
    "    (\"GaussianNB\", sklearn.naive_bayes.GaussianNB(), \"fast\"),\n",
    "    # crashes ('GaussianProcessClassifier', sklearn.gaussian_process.GaussianProcessClassifier(), 'slow'),\n",
    "    (\"GradientBoostingClassifier\", sklearn.ensemble.GradientBoostingClassifier(), \"slow\"),\n",
    "    (\"HistGradientBoostingClassifier\", sklearn.ensemble.HistGradientBoostingClassifier(), \"slow\"),\n",
    "    (\"KNeighborsClassifier\", sklearn.neighbors.KNeighborsClassifier(n_jobs=1), \"fast\"),\n",
    "    # ('LabelPropagation', sklearn.semi_supervised.LabelPropagation(), 'slow'),  # requires too much memory to train with larger datasets\n",
    "    # ('LabelSpreading', sklearn.semi_supervised.LabelSpreading(), 'slow'),  # bit slow\n",
    "    (\"LinearDiscriminantAnalysis\", sklearn.discriminant_analysis.LinearDiscriminantAnalysis(), \"fast\"),\n",
    "    (\"LinearSVC\", sklearn.svm.LinearSVC(max_iter=20000), \"slow\"),  # slow with unscaled data\n",
    "    (\n",
    "        \"LogisticRegression\",\n",
    "        sklearn.linear_model.LogisticRegression(solver=\"lbfgs\", multi_class=\"auto\", max_iter=20000),\n",
    "        \"slow\",\n",
    "    ),  # slow with unscaled data\n",
    "    (\n",
    "        \"LogisticRegressionCV\",\n",
    "        sklearn.linear_model.LogisticRegressionCV(cv=5, solver=\"lbfgs\", multi_class=\"auto\", max_iter=20000),\n",
    "        \"slow\",\n",
    "    ),\n",
    "    (\"MLPClassifier\", sklearn.neural_network.MLPClassifier(), \"slow\"),\n",
    "    (\"MultinomialNB\", sklearn.naive_bayes.MultinomialNB(), \"fast\"),\n",
    "    (\"NearestCentroid\", sklearn.neighbors.NearestCentroid(), \"fast\"),\n",
    "    # nu infeasible ('NuSVC', sklearn.svm.NuSVC(), 'fast'),\n",
    "    (\n",
    "        \"PassiveAggressiveClassifier\",\n",
    "        sklearn.linear_model.PassiveAggressiveClassifier(max_iter=1000, tol=1e-3, n_jobs=1),\n",
    "        \"fast\",\n",
    "    ),\n",
    "    (\"Perceptron\", sklearn.linear_model.Perceptron(n_jobs=-1), \"fast\"),\n",
    "    (\"QuadraticDiscriminantAnalysis\", sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis(), \"fast\"),\n",
    "    (\n",
    "        \"RadiusNeighborsClassifier\",\n",
    "        sklearn.neighbors.RadiusNeighborsClassifier(radius=2, outlier_label=0, n_jobs=1),\n",
    "        \"fast\",\n",
    "    ),\n",
    "    (\"RandomForestClassifier\", sklearn.ensemble.RandomForestClassifier(n_estimators=100, n_jobs=1), \"medi\"),\n",
    "    (\"RidgeClassifier\", sklearn.linear_model.RidgeClassifier(), \"fast\"),\n",
    "    (\"RidgeClassifierCV\", sklearn.linear_model.RidgeClassifierCV(), \"fast\"),\n",
    "    (\"SGDClassifier\", sklearn.linear_model.SGDClassifier(max_iter=1000, tol=1e-3, n_jobs=1), \"medi\"),\n",
    "    (\"SVC\", sklearn.svm.SVC(gamma=\"scale\"), \"slow\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some models only work with properly scaled data, so we prepare ALL available scalers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = [\n",
    "    # ('Unscaled data', ),\n",
    "    (\"standard scaling\", sklearn.preprocessing.StandardScaler()),\n",
    "    (\"min-max scaling\", sklearn.preprocessing.MinMaxScaler()),\n",
    "    (\"max-abs scaling\", sklearn.preprocessing.MaxAbsScaler()),\n",
    "    (\"robust scaling\", sklearn.preprocessing.RobustScaler(quantile_range=(25, 75))),\n",
    "    (\"power transformation (Yeo-Johnson)\", sklearn.preprocessing.PowerTransformer(method=\"yeo-johnson\")),\n",
    "    # ('power transformation (Box-Cox)', sklearn.preprocessing.PowerTransformer(method='box-cox')), # 'strictly zero' meh.\n",
    "    (\"quantile transformation (gaussian pdf)\", sklearn.preprocessing.QuantileTransformer(output_distribution=\"normal\")),\n",
    "    (\"quantile transformation (uniform pdf)\", sklearn.preprocessing.QuantileTransformer(output_distribution=\"uniform\")),\n",
    "    (\"sample-wise L2 normalizing\", sklearn.preprocessing.Normalizer()),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata, testdata = tridata(distance=15, doubleplane=30, energy=600, erel=500, nmax=nmax, physics=\"inclxx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and Test data is passed through the scalers, including the \"unscaled\" scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainscaled = [(\"Unscaled data\", traindata[[\"nHits\", \"nClus\", \"Edep\"]], testdata[[\"nHits\", \"nClus\", \"Edep\"]],)] + [\n",
    "    (\n",
    "        sname,\n",
    "        scaler.fit_transform(traindata[[\"nHits\", \"nClus\", \"Edep\"]]),\n",
    "        scaler.transform(testdata[[\"nHits\", \"nClus\", \"Edep\"]]),\n",
    "    )\n",
    "    for sname, scaler in scalers\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run all model/scaler combinations, in parallel. Note that we use timeouts per task, as setting at timeout in joblib will throw everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEDIHEAD = 500000\n",
    "SLOWHEAD = 200000\n",
    "\n",
    "\n",
    "@with_timeout(1200 * 2)\n",
    "def train_model(mname, modelorg, speed, sname, x_train, x_test):\n",
    "    # These get killed without error?\n",
    "    if mname == \"RadiusNeighborsClassifier\" and sname != \"Unscaled data\":\n",
    "        return (mname, sname, np.NaN, speed, np.NaN, \"Skipped\")\n",
    "    try:\n",
    "        model = sklearn.base.clone(modelorg)\n",
    "        start = time.time()\n",
    "        if speed == \"slow\":\n",
    "            model.fit(x_train[0:SLOWHEAD], traindata.head(SLOWHEAD)[[label]].values.ravel())\n",
    "        elif speed == \"medi\":\n",
    "            model.fit(x_train[0:MEDIHEAD], traindata.head(MEDIHEAD)[[label]].values.ravel())\n",
    "        elif speed == \"fast\":\n",
    "            model.fit(x_train, traindata[[label]].values.ravel())\n",
    "        end = time.time()\n",
    "\n",
    "        y_pred = model.predict(x_test)\n",
    "        y_true = testdata[[label]].values.ravel()\n",
    "\n",
    "        bac = sklearn.metrics.balanced_accuracy_score(y_true, y_pred)\n",
    "        return (mname, sname, bac, speed, (end - start), \"ok\")\n",
    "    except Exception as err:\n",
    "        return (mname, sname, np.NaN, speed, np.NaN, err)\n",
    "\n",
    "\n",
    "def train_model_wrap(mname, modelorg, speed, sname, x_train, x_test):\n",
    "    ret = train_model(mname, modelorg, speed, sname, x_train, x_test)\n",
    "    if ret:\n",
    "        return ret\n",
    "    else:\n",
    "        return (mname, sname, np.NaN, speed, np.NaN, \"Timeout\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = Parallel(n_jobs=4)(\n",
    "    delayed(train_model_wrap)(mname, modelorg, speed, sname, x_train, x_test)\n",
    "    for sname, x_train, x_test in trainscaled\n",
    "    for mname, modelorg, speed in models\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsdf = pd.DataFrame(results)\n",
    "pd.options.display.max_rows = 999\n",
    "resultsdf.columns = [\"Model\", \"Scaler\", \"BAC\", \"Speed\", \"Time\", \"Status\"]\n",
    "resultsdf.sort_values(by=[\"BAC\", \"Time\", \"Model\"], ascending=[False, True, True], inplace=True)\n",
    "resultsdf.style.hide_index().format({\"BAC\": \"{:.2%}\", \"Time\": \"{:.2f}\"}).bar(subset=[\"BAC\"], color=\"lightgreen\").bar(\n",
    "    subset=[\"Time\"], color=\"lightblue\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
