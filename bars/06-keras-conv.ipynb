{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.preprocessing\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from helpers import filename_for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow 2.3.1\n",
      "keras 2.4.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "\n",
    "print(\"tensorflow\", tensorflow.__version__)\n",
    "print(\"keras\", keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGeneratorBars(keras.utils.Sequence):\n",
    "    def __init__(self, config):\n",
    "        self.c = config\n",
    "\n",
    "        self.labels = []\n",
    "        self.featuresTri = []\n",
    "        self.featuresBars = []\n",
    "\n",
    "        self.scaler_tri = sklearn.preprocessing.MaxAbsScaler()\n",
    "        self.scaler_e = sklearn.preprocessing.MaxAbsScaler()\n",
    "        self.scaler_t = sklearn.preprocessing.MaxAbsScaler()\n",
    "\n",
    "        file = filename_for(\n",
    "            self.c[\"distance\"],\n",
    "            self.c[\"doubleplane\"],\n",
    "            self.c[\"energy\"],\n",
    "            self.c[\"erel\"],\n",
    "            self.c[\"neutrons\"][0],\n",
    "            \"inclxx\",\n",
    "            self.c[\"subruns\"][0],\n",
    "            \"bars.parquet\",\n",
    "        )\n",
    "        data = pd.read_parquet(file)\n",
    "        rows = len(data.index)\n",
    "        del data\n",
    "\n",
    "        self.batches_per_subrun = (rows * len(self.c[\"neutrons\"])) // self.c[\"batch_size\"]\n",
    "        self.batches_per_cache = self.batches_per_subrun * self.c[\"subrun_cache_size\"]\n",
    "        self.len = self.batches_per_subrun * len(self.c[\"subruns\"])\n",
    "\n",
    "        self.cache_subruns = [\n",
    "            self.c[\"subruns\"][i : i + self.c[\"subrun_cache_size\"]]\n",
    "            for i in range(0, len(self.c[\"subruns\"]), self.c[\"subrun_cache_size\"])\n",
    "        ]\n",
    "        self.current_cache = -1\n",
    "\n",
    "        print(f\"Rows in one file: {rows}\")\n",
    "        print(f\"{self.batches_per_subrun} batches per subrun\")\n",
    "        print(f\"{self.len} total batches in {self.cache_subruns} caches\")\n",
    "\n",
    "        self.fitscalers()\n",
    "        self.load(0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        cacheid = index // self.batches_per_cache\n",
    "        i = index % (self.batches_per_cache)\n",
    "        # print(f\"{index} -> c{cacheid}-i{i}\")\n",
    "\n",
    "        if cacheid != self.current_cache:\n",
    "            self.load(cacheid)\n",
    "\n",
    "        a = i * self.c[\"batch_size\"]\n",
    "        b = (i + 1) * self.c[\"batch_size\"]\n",
    "\n",
    "        tri = self.featuresTri[a:b]\n",
    "        bars = self.featuresBars[a:b]\n",
    "        x = bars  # [tri, bars]\n",
    "        y = self.labels[a:b]\n",
    "        return x, y\n",
    "\n",
    "    def load(self, cacheid):\n",
    "        subruns = self.cache_subruns[cacheid]\n",
    "        print(f\"Loading subruns {subruns} for cache {cacheid}\")\n",
    "\n",
    "        files = [\n",
    "            filename_for(\n",
    "                self.c[\"distance\"],\n",
    "                self.c[\"doubleplane\"],\n",
    "                self.c[\"energy\"],\n",
    "                self.c[\"erel\"],\n",
    "                n,\n",
    "                \"inclxx\",\n",
    "                subrun,\n",
    "                \"bars.parquet\",\n",
    "            )\n",
    "            for n in self.c[\"neutrons\"]\n",
    "            for subrun in subruns\n",
    "        ]\n",
    "        data = pd.concat([pd.read_parquet(file) for file in files], ignore_index=True).sample(frac=1)\n",
    "        data.loc[data[\"nHits\"] == 0, self.c[\"label\"]] = 0\n",
    "\n",
    "        self.current_cache = cacheid\n",
    "        self.featuresTri = self.scaler_tri.transform(data[self.c[\"cols_tri\"]])\n",
    "        self.featuresBars = np.dstack(\n",
    "            (\n",
    "                self.scaler_e.transform(data[self.c[\"cols_e\"]].values.reshape(-1, 1)).reshape(\n",
    "                    -1, len(self.c[\"cols_e\"])\n",
    "                ),\n",
    "                self.scaler_t.transform(data[self.c[\"cols_t\"]].values.reshape(-1, 1)).reshape(\n",
    "                    -1, len(self.c[\"cols_t\"])\n",
    "                ),\n",
    "            )\n",
    "        ).reshape(-1, 60, 50, 2)\n",
    "        self.labels = keras.utils.to_categorical(\n",
    "            data[[self.c[\"label\"]]].values.ravel(), num_classes=len(self.c[\"neutrons\"]) + 1\n",
    "        )\n",
    "        del data\n",
    "\n",
    "    def fitscalers(self):\n",
    "        subruns = range(5)  # self.cache_subruns[0]\n",
    "        files = [\n",
    "            filename_for(\n",
    "                self.c[\"distance\"],\n",
    "                self.c[\"doubleplane\"],\n",
    "                self.c[\"energy\"],\n",
    "                self.c[\"erel\"],\n",
    "                n,\n",
    "                \"inclxx\",\n",
    "                subrun,\n",
    "                \"bars.parquet\",\n",
    "            )\n",
    "            for n in self.c[\"neutrons\"]\n",
    "            for subrun in subruns\n",
    "        ]\n",
    "        data = pd.concat([pd.read_parquet(file) for file in files], ignore_index=True)\n",
    "        self.scaler_tri.fit(data[self.c[\"cols_tri\"]])\n",
    "        self.scaler_e.fit(data[self.c[\"cols_e\"]].values.reshape(-1, 1))\n",
    "        self.scaler_t.fit(data[self.c[\"cols_t\"]].values.reshape(-1, 1))\n",
    "        del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"distance\": 15,\n",
    "    \"doubleplane\": 30,\n",
    "    \"energy\": 600,\n",
    "    \"erel\": 500,\n",
    "    \"neutrons\": [1, 2, 3, 4],\n",
    "    \"subruns\": range(7),  # range(19),\n",
    "    \"subrun_cache_size\": 7,\n",
    "    \"batch_size\": 200,\n",
    "    \"cols_tri\": [\"nHits\", \"nClus\", \"Edep\"],\n",
    "    \"cols_e\": [str(i) for i in range(0, 30 * 100 * 2, 2)],\n",
    "    \"cols_t\": [str(i + 1) for i in range(0, 30 * 100 * 2, 2)],\n",
    "    \"label\": \"nPN\",\n",
    "}\n",
    "\n",
    "validation_config = config.copy()\n",
    "validation_config[\"subruns\"] = [19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in one file: 10000\n",
      "200 batches per subrun\n",
      "1400 total batches in [range(0, 7)] caches\n",
      "Loading subruns range(0, 7) for cache 0\n",
      "Rows in one file: 10000\n",
      "200 batches per subrun\n",
      "200 total batches in [[19]] caches\n",
      "Loading subruns [19] for cache 0\n"
     ]
    }
   ],
   "source": [
    "generator = DataGeneratorBars(config)\n",
    "validation_generator = DataGeneratorBars(validation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c2d-1 (None, 60, 50, 16)\n",
      "mp2d-1 (None, 60, 10, 16)\n",
      "c2d-2 (None, 58, 8, 32)\n",
      "f (None, 14848)\n",
      "d-1 (None, 2500)\n",
      "d-2 (None, 100)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import (\n",
    "    Activation,\n",
    "    BatchNormalization,\n",
    "    Conv2D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    "    Input,\n",
    "    MaxPooling2D,\n",
    "    concatenate,\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# define two sets of inputs\n",
    "# inputTri = Input(shape=(3,))\n",
    "inputBars = Input(shape=(60, 50, 2))\n",
    "\n",
    "# # the first branch operates on the first input\n",
    "# t = Dense(40, activation=\"relu\")(inputTri)\n",
    "# t = Dense(5, activation=\"relu\")(t)\n",
    "# t = Model(inputs=inputTri, outputs=t)\n",
    "\n",
    "# # the second branch opreates on the second input\n",
    "# b = Conv2D(200, (3,3))(inputBars)\n",
    "# #b = Dense(1000, activation=\"relu\")(inputBars)\n",
    "# b = Dense(10000, activation=\"relu\")(b)\n",
    "# #b = Dense(250, activation=\"relu\")(b)\n",
    "# b = Model(inputs=inputBars, outputs=b)\n",
    "\n",
    "# # CONV => RELU => BN => POOL\n",
    "# x = Conv2D(16, (3, 1), padding=\"valid\", activation=\"relu\")(inputBars)\n",
    "# #x = BatchNormalization(axis=-1)(x)\n",
    "# #x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# # flatten the volume, then FC => RELU => BN => DROPOUT\n",
    "# x = Flatten()(x)\n",
    "# x = Dense(600, activation=\"relu\")(x)\n",
    "# x = BatchNormalization(axis=-1)(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "\n",
    "# x = Dense(5)(x)\n",
    "# x = Activation(\"relu\")(x)\n",
    "\n",
    "# # combine the output of the two branches\n",
    "# combined = concatenate([t.output, x.output])\n",
    "# # apply a FC layer and then a regression prediction on the\n",
    "# # combined outputs\n",
    "# z = Dense(50, activation=\"relu\")(combined)\n",
    "# # z = Dense(100, activation=\"relu\")(z)\n",
    "# z = Dense(len(config[\"neutrons\"]) + 1, activation=\"softmax\")(z)\n",
    "\n",
    "x = inputBars\n",
    "chanDim = -1\n",
    "# loop over the number of filters\n",
    "#for i, (f, s) in enumerate([(5, (3, 1)), (10, (3, 3)), ]): -> 0,67\n",
    "#for i, (f, s) in enumerate([(3, (1, 5)), (3, (3, 3)), ]):\n",
    "\n",
    "\n",
    "x = Conv2D(16, (1, 3), padding=\"same\", activation=\"relu\")(x)\n",
    "print(\"c2d-1\", x.shape)\n",
    "#x = BatchNormalization(axis=chanDim)(x)\n",
    "#x = Dropout(0.2)(x)\n",
    "x = MaxPooling2D(pool_size=(1, 5))(x)\n",
    "print(\"mp2d-1\", x.shape)\n",
    "\n",
    "x = Conv2D(32, (3, 3), padding=\"valid\", activation=\"relu\")(x)\n",
    "print(\"c2d-2\", x.shape)\n",
    "#x = BatchNormalization(axis=chanDim)(x)\n",
    "#x = Dropout(0.2)(x)\n",
    "#x = MaxPooling2D(pool_size=(5, 5))(x)\n",
    "#print(\"mp2d-2\", x.shape)\n",
    "\n",
    "x = Flatten()(x)\n",
    "print(\"f\", x.shape)\n",
    "x = Dense(2500, activation=\"relu\")(x)\n",
    "print(\"d-1\", x.shape)\n",
    "x = Dense(100, activation=\"relu\")(x)\n",
    "print(\"d-2\", x.shape)\n",
    "# x = BatchNormalization(axis=chanDim)(x)\n",
    "\n",
    "x = Dense(len(config[\"neutrons\"]) + 1, activation=\"softmax\")(x)\n",
    "model = Model(inputBars, x)\n",
    "# model = Model(inputs=[t.input, x.input], outputs=z)\n",
    "loss = keras.losses.CategoricalCrossentropy()\n",
    "optimizer = keras.optimizers.Adagrad()\n",
    "#model.compile(loss=loss, optimizer=\"adagrad\", metrics=[\"accuracy\"])\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-df40928c593b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\janma\\anaconda3\\envs\\neuland38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\janma\\anaconda3\\envs\\neuland38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1047\u001b[0m          \u001b[0mtraining_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRespectCompiledTrainableState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m       \u001b[1;31m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1049\u001b[1;33m       data_handler = data_adapter.DataHandler(\n\u001b[0m\u001b[0;32m   1050\u001b[0m           \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1051\u001b[0m           \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\janma\\anaconda3\\envs\\neuland38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[0;32m   1103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1105\u001b[1;33m     self._adapter = adapter_cls(\n\u001b[0m\u001b[0;32m   1106\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1107\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\janma\\anaconda3\\envs\\neuland38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, shuffle, workers, use_multiprocessing, max_queue_size, model, **kwargs)\u001b[0m\n\u001b[0;32m    907\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_keras_sequence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    908\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_enqueuer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 909\u001b[1;33m     super(KerasSequenceAdapter, self).__init__(\n\u001b[0m\u001b[0;32m    910\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Shuffle is handed in the _make_callable override.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\janma\\anaconda3\\envs\\neuland38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, workers, use_multiprocessing, max_queue_size, model, **kwargs)\u001b[0m\n\u001b[0;32m    803\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    804\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 805\u001b[1;33m     \u001b[0moutput_shapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_get_dynamic_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpeek\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    806\u001b[0m     \u001b[0moutput_types\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpeek\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    807\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\janma\\anaconda3\\envs\\neuland38\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 635\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    637\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\janma\\anaconda3\\envs\\neuland38\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 635\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    637\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\janma\\anaconda3\\envs\\neuland38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m_get_dynamic_shape\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    797\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    798\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_dynamic_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 799\u001b[1;33m       \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    800\u001b[0m       \u001b[1;31m# Unknown number of dimensions, `as_list` cannot be called.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    801\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "history = model.fit(generator, verbose=2, epochs=20, shuffle=False, validation_data=validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del generator\n",
    "del validation_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_config = config.copy()\n",
    "test_config[\"subruns\"] = range(7, 11)\n",
    "test_generator = DataGeneratorBars(test_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = [test_generator.featuresTri, test_generator.featuresBars]\n",
    "X = test_generator.featuresBars\n",
    "y_true = np.argmax(test_generator.labels, axis=1)\n",
    "y_pred = np.argmax(model.predict(X), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    balanced_accuracy_score,\n",
    "    confusion_matrix,\n",
    "    plot_confusion_matrix,\n",
    ")\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "bac = balanced_accuracy_score(y_true, y_pred)\n",
    "print(bac)\n",
    "cm = confusion_matrix(y_true, y_pred, labels=range(0, len(config[\"neutrons\"]) + 1))\n",
    "print(cm)\n",
    "cmrel = confusion_matrix(y_true, y_pred, labels=range(0, len(config[\"neutrons\"]) + 1), normalize=\"true\")\n",
    "print((cmrel * 100).round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history[\"accuracy\"])\n",
    "plt.plot(history.history[\"val_accuracy\"])\n",
    "plt.title(\"Model accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Train\", \"Test\"], loc=\"upper left\")\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history[\"loss\"])\n",
    "# plt.plot(history.history[\"val_loss\"])\n",
    "plt.title(\"Model loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Train\", \"Test\"], loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
