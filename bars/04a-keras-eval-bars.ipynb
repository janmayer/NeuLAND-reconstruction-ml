{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.preprocessing\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from helpers import filename_for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow 2.1.0\n",
      "keras 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "\n",
    "print(\"tensorflow\", tensorflow.__version__)\n",
    "print(\"keras\", keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGeneratorBars(keras.utils.Sequence):\n",
    "    def __init__(self, config):\n",
    "        self.c = config\n",
    "\n",
    "        self.labels = []\n",
    "        self.features = []\n",
    "\n",
    "        self.scaler_tri = sklearn.preprocessing.MaxAbsScaler()\n",
    "        self.scaler_e = sklearn.preprocessing.MaxAbsScaler()\n",
    "        self.scaler_t = sklearn.preprocessing.MaxAbsScaler()\n",
    "\n",
    "        file = filename_for(\n",
    "            self.c[\"distance\"],\n",
    "            self.c[\"doubleplane\"],\n",
    "            self.c[\"energy\"],\n",
    "            self.c[\"erel\"],\n",
    "            self.c[\"neutrons\"][0],\n",
    "            \"inclxx\",\n",
    "            self.c[\"subruns\"][0],\n",
    "            \"bars.parquet\",\n",
    "        )\n",
    "        data = pd.read_parquet(file)\n",
    "        rows = len(data.index)\n",
    "        del data\n",
    "\n",
    "        self.batches_per_subrun = (rows * len(self.c[\"neutrons\"])) // self.c[\"batch_size\"]\n",
    "        self.batches_per_cache = self.batches_per_subrun * self.c[\"subrun_cache_size\"]\n",
    "        self.len = self.batches_per_subrun * len(self.c[\"subruns\"])\n",
    "\n",
    "        self.cache_subruns = [\n",
    "            self.c[\"subruns\"][i : i + self.c[\"subrun_cache_size\"]]\n",
    "            for i in range(0, len(self.c[\"subruns\"]), self.c[\"subrun_cache_size\"])\n",
    "        ]\n",
    "        self.current_cache = -1\n",
    "\n",
    "        print(f\"Rows in one file: {rows}\")\n",
    "        print(f\"{self.batches_per_subrun} batches per subrun\")\n",
    "        print(f\"{self.len} total batches in {self.cache_subruns} caches\")\n",
    "\n",
    "        self.fitscalers()\n",
    "        self.load(0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        cacheid = index // self.batches_per_cache\n",
    "        i = index % (self.batches_per_cache)\n",
    "        # print(f\"{index} -> c{cacheid}-i{i}\")\n",
    "\n",
    "        if cacheid != self.current_cache:\n",
    "            self.load(cacheid)\n",
    "\n",
    "        a = i * self.c[\"batch_size\"]\n",
    "        b = (i + 1) * self.c[\"batch_size\"]\n",
    "\n",
    "        x = self.features[a:b]\n",
    "        y = self.labels[a:b]\n",
    "        return x, y, [None]\n",
    "\n",
    "    def load(self, cacheid):\n",
    "        subruns = self.cache_subruns[cacheid]\n",
    "        print(f\"Loading subruns {subruns} for cache {cacheid}\")\n",
    "\n",
    "        files = [\n",
    "            filename_for(\n",
    "                self.c[\"distance\"],\n",
    "                self.c[\"doubleplane\"],\n",
    "                self.c[\"energy\"],\n",
    "                self.c[\"erel\"],\n",
    "                n,\n",
    "                \"inclxx\",\n",
    "                subrun,\n",
    "                \"bars.parquet\",\n",
    "            )\n",
    "            for n in self.c[\"neutrons\"]\n",
    "            for subrun in subruns\n",
    "        ]\n",
    "        data = pd.concat([pd.read_parquet(file) for file in files], ignore_index=True).sample(frac=1)\n",
    "        data.loc[data[\"nHits\"] == 0, self.c[\"label\"]] = 0\n",
    "\n",
    "        self.current_cache = cacheid\n",
    "        self.features = np.concatenate(\n",
    "            (\n",
    "                # self.scaler_tri.transform(data[self.c[\"cols_tri\"]]),\n",
    "                self.scaler_e.transform(data[self.c[\"cols_e\"]].values.reshape(-1, 1)).reshape(\n",
    "                    -1, len(self.c[\"cols_e\"])\n",
    "                ),\n",
    "                self.scaler_t.transform(data[self.c[\"cols_t\"]].values.reshape(-1, 1)).reshape(\n",
    "                    -1, len(self.c[\"cols_t\"])\n",
    "                ),\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "        self.labels = keras.utils.to_categorical(\n",
    "            data[[self.c[\"label\"]]].values.ravel(), num_classes=len(self.c[\"neutrons\"]) + 1\n",
    "        )\n",
    "        del data\n",
    "\n",
    "    def fitscalers(self):\n",
    "        subruns = range(5)  # self.cache_subruns[0]\n",
    "        files = [\n",
    "            filename_for(\n",
    "                self.c[\"distance\"],\n",
    "                self.c[\"doubleplane\"],\n",
    "                self.c[\"energy\"],\n",
    "                self.c[\"erel\"],\n",
    "                n,\n",
    "                \"inclxx\",\n",
    "                subrun,\n",
    "                \"bars.parquet\",\n",
    "            )\n",
    "            for n in self.c[\"neutrons\"]\n",
    "            for subrun in subruns\n",
    "        ]\n",
    "        data = pd.concat([pd.read_parquet(file) for file in files], ignore_index=True)\n",
    "        # self.scaler_tri.fit(data[self.c[\"cols_tri\"]])\n",
    "        self.scaler_e.fit(data[self.c[\"cols_e\"]].values.reshape(-1, 1))\n",
    "        self.scaler_t.fit(data[self.c[\"cols_t\"]].values.reshape(-1, 1))\n",
    "        del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"distance\": 15,\n",
    "    \"doubleplane\": 30,\n",
    "    \"energy\": 600,\n",
    "    \"erel\": 500,\n",
    "    \"neutrons\": [1, 2, 3, 4],\n",
    "    \"subruns\": range(7),  # range(19),\n",
    "    \"subrun_cache_size\": 7,\n",
    "    \"batch_size\": 200,\n",
    "    # \"cols_tri\": [\"nHits\", \"nClus\", \"Edep\"],\n",
    "    \"cols_e\": [str(i) for i in range(0, 30 * 100 * 2, 2)],\n",
    "    \"cols_t\": [str(i + 1) for i in range(0, 30 * 100 * 2, 2)],\n",
    "    \"label\": \"nPN\",\n",
    "}\n",
    "\n",
    "validation_config = config.copy()\n",
    "validation_config[\"subruns\"] = [19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in one file: 10000\n",
      "200 batches per subrun\n",
      "1400 total batches in [range(0, 7)] caches\n",
      "Loading subruns range(0, 7) for cache 0\n",
      "Rows in one file: 10000\n",
      "200 batches per subrun\n",
      "200 total batches in [[19]] caches\n",
      "Loading subruns [19] for cache 0\n"
     ]
    }
   ],
   "source": [
    "generator = DataGeneratorBars(config)\n",
    "validation_generator = DataGeneratorBars(validation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(units=20000, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(units=1000, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(units=len(config[\"neutrons\"]) + 1, activation=\"softmax\"))\n",
    "loss = keras.losses.CategoricalCrossentropy()\n",
    "model.compile(loss=loss, optimizer=\"adagrad\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 1400 steps, validate for 200 steps\n",
      "Epoch 1/10\n",
      "1400/1400 - 108s - loss: 0.8229 - accuracy: 0.6320 - val_loss: 0.7841 - val_accuracy: 0.6444\n",
      "Epoch 2/10\n",
      "1400/1400 - 105s - loss: 0.7685 - accuracy: 0.6491 - val_loss: 0.7698 - val_accuracy: 0.6463\n",
      "Epoch 3/10\n",
      "1400/1400 - 105s - loss: 0.7523 - accuracy: 0.6586 - val_loss: 0.7638 - val_accuracy: 0.6578\n",
      "Epoch 4/10\n",
      "1400/1400 - 103s - loss: 0.7410 - accuracy: 0.6696 - val_loss: 0.7600 - val_accuracy: 0.6572\n",
      "Epoch 5/10\n",
      "1400/1400 - 98s - loss: 0.7312 - accuracy: 0.6734 - val_loss: 0.7569 - val_accuracy: 0.6576\n",
      "Epoch 6/10\n",
      "1400/1400 - 98s - loss: 0.7221 - accuracy: 0.6775 - val_loss: 0.7543 - val_accuracy: 0.6581\n",
      "Epoch 7/10\n",
      "1400/1400 - 98s - loss: 0.7134 - accuracy: 0.6818 - val_loss: 0.7521 - val_accuracy: 0.6585\n",
      "Epoch 8/10\n",
      "1400/1400 - 98s - loss: 0.7050 - accuracy: 0.6862 - val_loss: 0.7502 - val_accuracy: 0.6596\n",
      "Epoch 9/10\n",
      "1400/1400 - 98s - loss: 0.6968 - accuracy: 0.6906 - val_loss: 0.7486 - val_accuracy: 0.6607\n",
      "Epoch 10/10\n",
      "1400/1400 - 98s - loss: 0.6890 - accuracy: 0.6951 - val_loss: 0.7473 - val_accuracy: 0.6610\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(generator, verbose=2, epochs=10, shuffle=False, validation_data=validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del generator\n",
    "del validation_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in one file: 10000\n",
      "200 batches per subrun\n",
      "800 total batches in [range(7, 11)] caches\n",
      "Loading subruns range(7, 11) for cache 0\n"
     ]
    }
   ],
   "source": [
    "test_config = config.copy()\n",
    "test_config[\"subruns\"] = range(7, 11)\n",
    "test_generator = DataGeneratorBars(test_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = test_generator.features\n",
    "y_true = np.argmax(test_generator.labels, axis=1)\n",
    "y_pred = np.argmax(model.predict(X), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7264430277466429\n",
      "[[ 2086     0     0     0     0]\n",
      " [  490 33736  3761    22    17]\n",
      " [   58  8916 24514  6277   127]\n",
      " [    4  1452 10866 19402  8272]\n",
      " [    0   191  2706 11286 25817]]\n",
      "[[100.   0.   0.   0.   0.]\n",
      " [  1.  89.  10.   0.   0.]\n",
      " [  0.  22.  61.  16.   0.]\n",
      " [  0.   4.  27.  49.  21.]\n",
      " [  0.   0.   7.  28.  65.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (\n",
    "    balanced_accuracy_score,\n",
    "    confusion_matrix,\n",
    "    plot_confusion_matrix,\n",
    ")\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "bac = balanced_accuracy_score(y_true, y_pred)\n",
    "print(bac)\n",
    "cm = confusion_matrix(y_true, y_pred, labels=range(0, len(config[\"neutrons\"]) + 1))\n",
    "print(cm)\n",
    "cmrel = confusion_matrix(y_true, y_pred, labels=range(0, len(config[\"neutrons\"]) + 1), normalize=\"true\")\n",
    "print((cmrel * 100).round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
